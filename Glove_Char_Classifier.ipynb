{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove-Char-Classifier\n",
    "\n",
    "This notebook is part of my tutorial on creating custom GloVe embeddings. Please find the repository here: https://github.com/remo-help/character-embedding-with-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "First we need to import sklearn and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the embeddings\n",
    "\n",
    "Unfortunately, sklearn by itself does not support GloVe vector files, so we need to write a little function that allows us to read in the embeddings we need.\n",
    "\n",
    "This function will take a file with glove-style embeddings and return a dictionary where the keys are the word or character that is embedded and the value is the respective vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove(path):\n",
    "        \n",
    "    embeddings_dict={}\n",
    "        \n",
    "    f = open(path,'r',encoding='utf8') #reading in the input data\n",
    "    vector_file = f.read()\n",
    "    f.close()\n",
    "    vector_file=vector_file.split(\"\\n\")\n",
    "        \n",
    "    for line in vector_file:\n",
    "        if line[0]:\n",
    "            line=line.split()\n",
    "            token = line[0]\n",
    "            vector=line[1:]\n",
    "            vector= [float(x) for x in vector]\n",
    "            vector = np.array(line[1:])\n",
    "            vector = vector.astype('float64')\n",
    "            embeddings_dict[token]=vector\n",
    "            \n",
    "    return embeddings_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classifier\n",
    "\n",
    "Next, we have to define the classifier that we intend to use. This classifier will have a variety of attributes and functions:\n",
    "\n",
    "### model\n",
    "We will use the sklearn LogisticRegression model.\n",
    "### label_encoder\n",
    "We will use the sklearn LabelEncoder\n",
    "### vectorizer\n",
    "For comparison, we will also use the sklearn CountVectorizer\n",
    "\n",
    "### train()\n",
    "This function will train our classifier on the GloVe vectors we provide.\n",
    "### train_count()\n",
    "This function will train our classifier on the embeddings provided by the CountVectorizer of the sklearn library. This is a count-based embedding technique.\n",
    "### predict_labels()\n",
    "This function will take in test data and make predictions based on the test data. The output of the function will be encoded labels (integers). We can inverse_transform those labels with the LabelEncoder if we want to see the strings.\n",
    "### predict_labels_count()\n",
    "This is the same as above, except for the CountVectorizer embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the classifier.\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "           \n",
    "        self.vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 10)) #we are using the \"word\" parameter because our characters are already seperated\n",
    "\n",
    "        \n",
    "        self.model = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=5000,verbose=1)\n",
    "    \n",
    "    def train(self, vectors, train_data_path):\n",
    "        \"\"\"\n",
    "        trains on the GloVe embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        f = open(train_data_path,'r',encoding=\"utf8\") #reading in the input data\n",
    "        input_string = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        input_list=input_string.split(\"\\n\") #storing each datum as a string in a list\n",
    "        input_list=input_list[0:-2] #getting rid of the last empty newline\n",
    "        feature_strings= []\n",
    "        label_strings= []\n",
    "        \n",
    "        for datum in input_list:\n",
    "            temp = datum.split(\"\\t\") #separating the word from its label\n",
    "            feature_strings.append(temp[0])\n",
    "            label_strings.append(temp[1])\n",
    "        del input_list #deleting the initial list to be economic\n",
    "        \n",
    "        vector_list=[] #here we will store our feature vectors\n",
    "        \n",
    "        for feature in feature_strings: #selecting a feature\n",
    "            feature = feature[0:-1].split(\" \") #getting rid of trailing space and splitting on space\n",
    "            temp_list = [] #here we will compile the feature vector\n",
    "            for char in feature: #iterating over characters of the word\n",
    "                if char in vectors.keys(): #making sure we are not running into unknown chars\n",
    "                    vector = vectors[char] #getting the vector associated with the character\n",
    "                    temp_list.append(vector)\n",
    "            \n",
    "            if len(temp_list)==0:\n",
    "                print(\"cannot find:\",feature)\n",
    "                \n",
    "            base = temp_list[0] #selecting the first character-vector\n",
    "\n",
    "            for i in range(1,len(temp_list)): \n",
    "                base=np.add(base,temp_list[i]) #adding all other character vectors item-wise\n",
    "            array=base/len(temp_list) #dividing by the total amount of characters\n",
    "        \n",
    "            vector_list.append(array) #putting the new and averaged array into our list\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        x_train = vector_list #transforming the feature strings into glove vectors based on our pretrained embedding\n",
    "\n",
    "        y_train = self.label_encoder.fit_transform(label_strings) #fitting and transforming the labels to integers\n",
    "        \n",
    "        if len(x_train)!=len(y_train): #making sure we have as many feature vectors as we have labels\n",
    "            print(\"features:\",len(x_train),\"\\n\",\"labels:\",len(y_train))\n",
    "        \n",
    "        self.model.fit(x_train, y_train)\n",
    "        \n",
    "        print(\"The classifier has finished training\")\n",
    "        \n",
    "    def train_count(self, train_data_path):\n",
    "        \"\"\"\n",
    "        This trains the classifier on vectors created by the CountVectorizer, as opposed to pretrained embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        f = open(train_data_path,'r',encoding=\"utf8\") #reading in the input data\n",
    "        input_string = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        input_list=input_string.split(\"\\n\") #storing each datum as a string in a list\n",
    "        input_list=input_list[0:-2] #getting rid of the last empty newline\n",
    "        feature_strings= []\n",
    "        label_strings= []\n",
    "        \n",
    "        for datum in input_list:\n",
    "            temp = datum.split(\"\\t\") #separating the word from its label\n",
    "            feature_strings.append(temp[0])\n",
    "            label_strings.append(temp[1])\n",
    "        del input_list #deleting the initial list\n",
    "        \n",
    "\n",
    "        \n",
    "        x_train = self.vectorizer.fit_transform(feature_strings) #transforming the feature strings into vectors with the count vectorizer\n",
    "\n",
    "        y_train = self.label_encoder.fit_transform(label_strings) #fitting and transforming the labels to integers\n",
    "        \n",
    "        self.model.fit(x_train, y_train) #training the model\n",
    "        \n",
    "        print(\"The classifier has finished training (CountVectorizer)\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict_labels(self, vectors, test_data_path):\n",
    "        \"\"\"\n",
    "        Takes a testfile where tokens and labels are seperated with \\t\n",
    "        returns the sequence of gold-lables and the sequence of predicted labels in INTEGER FORM\n",
    "        \"\"\"\n",
    "        f = open(test_data_path,'r',encoding=\"utf8\") #reading in the input data\n",
    "        input_string = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        input_list=input_string.split(\"\\n\") #storing each datum as a string in a list\n",
    "        input_list=input_list[0:-2] #getting rid of the last empty newline\n",
    "        feature_strings= []\n",
    "        label_strings= []\n",
    "        \n",
    "        for datum in input_list:\n",
    "            temp = datum.split(\"\\t\") #separating the word from its label\n",
    "            feature_strings.append(temp[0])\n",
    "            label_strings.append(temp[1])\n",
    "        del input_list #deleting the initial list to be economic\n",
    "        \n",
    "        vector_list=[] #here we will store our feature vectors\n",
    "        \n",
    "        for feature in feature_strings: #selecting a feature\n",
    "            feature = feature[0:-1].split(\" \") #getting rid of trailing space and splitting on space\n",
    "            temp_list = [] #here we will compile the feature vector\n",
    "            for char in feature: #iterating over characters of the word\n",
    "                if char in vectors.keys(): #making sure we are not running into unknown chars\n",
    "                    vector = vectors[char] #getting the vector associated with the character\n",
    "                    temp_list.append(vector)\n",
    "            \n",
    "            if len(temp_list)==0:\n",
    "                print(\"cannot find:\",feature)\n",
    "                \n",
    "            base = temp_list[0] #selecting the first character-vector\n",
    "\n",
    "            for i in range(1,len(temp_list)): \n",
    "                base=np.add(base,temp_list[i]) #adding all other character vectors item-wise\n",
    "            array=base/len(temp_list) #dividing by the total amount of characters\n",
    "        \n",
    "            vector_list.append(array) #putting the new and averaged array into our list\n",
    "        \n",
    "        x_test=vector_list\n",
    "\n",
    "        predictions = self.model.predict(x_test) #makes the predictions\n",
    "        \n",
    "        gold_labels = self.label_encoder.transform(label_strings)\n",
    "        \n",
    "        return predictions,gold_labels\n",
    "    \n",
    "    def predict_labels_count(self, test_data_path):\n",
    "        \"\"\"\n",
    "        Takes a testfile where tokens and labels are seperated with \\t\n",
    "        returns the sequence of gold-lables and the sequence of predicted labels in INTEGER FORM\n",
    "        This is the CountVectorizer based implementation\n",
    "        \"\"\"\n",
    "        f = open(test_data_path,'r') #reading in the test data\n",
    "        input_string = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        input_list=input_string.split(\"\\n\") #storing each datum as a string in a list\n",
    "        input_list=input_list[0:-2] #getting rid of the last empty newline\n",
    "        feature_strings= []\n",
    "        label_strings= []\n",
    "        \n",
    "        for datum in input_list:\n",
    "            temp = datum.split(\"\\t\") #separating the word from its label\n",
    "            feature_strings.append(temp[0])\n",
    "            label_strings.append(temp[1])\n",
    "        del input_list #deleting the initial list to be economic\n",
    "        \n",
    "        x_test = self.vectorizer.transform(feature_strings) #transforming the feature strings into vectors with the count vectorizer\n",
    "\n",
    "\n",
    "        predictions = self.model.predict(x_test) #makes the predictions\n",
    "        \n",
    "        gold_labels = self.label_encoder.transform(label_strings)\n",
    "        \n",
    "        return predictions,gold_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GloVe embeddings\n",
    "First we will train our classifier on our training data using the GloVe embeddings. For this we need to read in the embedding dictionary using glove() and then call classifier.train(). This may take a couple of minutes depending on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier has finished training\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "vec= glove('data/dickens_vectors.txt')\n",
    "classifier.train(vec,'data/train_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating GloVe embeddings with test data\n",
    "Now that we have trained the classifier with our custom embeddings, we see how it performs on the test data. The we store that result so we can later compare it to the performance with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict_labels(vec,\"data/test_tokens.txt\")\n",
    "\n",
    "glove_f1= f1_score(predictions[1], predictions[0], average='weighted')\n",
    "glove_accuracy=accuracy_score(predictions[1],predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CountVectorizer embeddings\n",
    "Now we will start another training sequence, this time based on the CountVectorizer embeddings. This will completely refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier has finished training (CountVectorizer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "classifier.train_count('data/count_train_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the CountVectorizer with the test data\n",
    "Now we let the newly trained classifier make predictions on the test data again. This time on the basis of the CountVectorizer embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict_labels_count(\"data/count_test_tokens.txt\")\n",
    "count_accuracy=accuracy_score(predictions[1],predictions[0])\n",
    "count_f1=f1_score(predictions[1], predictions[0], average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the GloVe implementation to the CountVectorizer\n",
    "Finally, let us compare the two and see which one performs better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results of the GloVe embeddings:\n",
      " F1_score:  0.6131187454146257 \n",
      " accuracy:  0.5547794117647059 \n",
      "\n",
      "Here are the results of the CountVectorizer embeddings:\n",
      " F1_score:  0.37002266016066365 \n",
      " accuracy:  0.3207424098671727\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are the results of the GloVe embeddings:\\n\",\"F1_score: \",glove_f1,\"\\n\", \"accuracy: \", glove_accuracy,\"\\n\")\n",
    "print(\"Here are the results of the CountVectorizer embeddings:\\n\",\"F1_score: \",count_f1,\"\\n\", \"accuracy: \", count_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the GloVe implementation performs much better. However, even the GloVe implementation does not yield amazing results. This is likely due to the fact that this is quite the difficult task. German, French, and English share many cognates (words that have similar roots). Also, the data contains a variety of names, which are a huge problem. For example \"Irene\" can be a name in French, English, and German. Considering these problems, the GloVe performance is still pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
